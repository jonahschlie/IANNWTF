{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\"mnist\", split=\"train\")\n",
    "ds = ds.map(lambda feature_dict: (feature_dict[\"image\"], feature_dict[\"label\"]))\n",
    "ds = ds.map(lambda image, label: (tf.reshape(image, (-1,)), label))\n",
    "ds = ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label))\n",
    "ds = ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
    "ds = ds.shuffle(2000).batch(200)\n",
    "ds = ds.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(200, 784), dtype=float32, numpy=\n",
      "array([[-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       ...,\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.]], dtype=float32)>, <tf.Tensor: shape=(200, 10), dtype=float32, numpy=\n",
      "array([[0., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 15:41:54.484133: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for elem in ds.take(1):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation via Subclassing from tf.keras.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, output_size=10):\n",
    "        super().__init__()\n",
    "        self.layers_list = []\n",
    "        # layer_sizes e.g. [256,256]\n",
    "        for layer_size in layer_sizes:\n",
    "            new_layer = tf.keras.layers.Dense(units=layer_size, activation=\"sigmoid\")\n",
    "            self.layers_list.append(new_layer)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=output_size, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "        y = self.output_layer(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_Model(layer_sizes=[256,256])\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "opt = tf.keras.optimizers.legacy.SGD()\n",
    "ds = ds\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    for x, target in ds:\n",
    "        with tf.GradientTape() as tape: # context manager ('with' opens context)\n",
    "            pred = model(x)\n",
    "            loss = cce(y_true=target, y_pred=pred)\n",
    "        grad = tape.gradient(loss, model.variables)\n",
    "        opt.apply_gradients(zip(grad, model.variables))\n",
    "        losses.append(loss.numpy())\n",
    "    print(np.mean(losses))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
